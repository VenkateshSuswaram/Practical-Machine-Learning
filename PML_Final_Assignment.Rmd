---
title: "Practical Machine Learning Final Assignment"
author: "Venkatesh"
date: "9 July 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively.
These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, oour goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

# Data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

# Analysis

## Appraoch

* Load the data set and briefly learn the characteristics of the data
* Use cross-validation method to built a valid model; 60% of the original data is used for model building (training data) while the rest of 40% of the data is used for testing (testing data)
* CLean the data by considering quality data.  
* Apply PCA to reduce the number of variables
* Apply random forest method to build a model
* Check the model with the testing data set - Cross validation
* Apply the model to estimate classes of 20 observations

An overall pseudo-random number generator seed was set at 5 for all code


## Library & Seed

```{r warning = FALSE, error = FALSE,message=FALSE}
library(knitr)
library(caret)
library(randomForest)
library(rpart)
library(RColorBrewer)
library(rattle)
library(rpart.plot)
set.seed(5)

```

## Loading data

```{r}
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))

dim(training)
dim(testing)

```

## Partioning the Training data set in to Train & test

We will split the training data in to 60 - 40 ratio , so that we can validate our model with the Training- Test data, before proceeding with test data.

```{r}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining)
dim(myTesting)
```

## Data Manupulation

Now we will remove the columns where NA % is greater than 60. This is because so many NA values in column will not produce fruitful results while using for prediction.

```{r}
myTraining <- myTraining[c(-1)]

x <- apply(myTraining ,2, function(y) sum(is.na(y))) / nrow(myTraining)
myTraining <- myTraining[!(x > 0.6)]

```

Make the test & training data to be in same format.

```{r}
Header1 = colnames(myTraining)
Header2 = colnames(myTraining[1,-59])
myTesting <- myTesting[Header1]

testing <- testing[c(-1)]
testing <- testing[Header2]


testing <- rbind(myTesting[1,1:58],testing)
testing <- testing[-1,]

```

## PCA

We have 50+ columns which will be cumbersome sometimes to do analysis. Hence we will use PCA technique to get the uncorrelated columns for predicting variables in the dataset.

```{r}

preProc <- preProcess(myTraining[,1:59],method="pca",thresh=.8) 
# PCA needed 15 components to capture 80 percent of the variance

preProc <- preProcess(myTraining[,1:59],method="pca",thresh=.95) 
#PCA needed 28 components to capture 95 percent of the variance

preProc <- preProcess(myTraining[,1:59],method="pca",pcaComp=28)

myTrainingPCA <- predict(preProc,myTraining[,1:59])

```


## Trees

```{r}
modFitA1 <- rpart(myTrainingPCA$classe ~ ., data=myTraining, method="class")

fancyRpartPlot(modFitA1)

```

Now we will compare the out of sample error using cross validation method.

## Predictions with Testing data set.- Cross Validation

```{r}
predictionsA1 <- predict(modFitA1, myTesting, type = "class")

confusionMatrix(predictionsA1, myTesting$classe)

```


## Random Forests


```{r}

modFitB1 <- randomForest(myTrainingPCA$classe ~. , data=myTraining)

predictionsB1 <- predict(modFitB1, myTesting, type = "class")

confusionMatrix(predictionsB1, myTesting$classe)


```

This model appears to be more robust than previous model. Hence we will proceed with this model to clacluate classe for training data set given.

```{r}

predictionsB2 <- predict(modFitB1, testing, type = "class")

```

## Export the results in to separate text files

```{r}
Fn_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

Fn_write_files(predictionsB2)
```


